= Observing models
:imagesdir: ../assets/images

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
<style>
  .nav-container, .pagination, .toolbar {
    display: none !important;
  }
  .doc {
    max-width: 70rem !important;
  }
</style>
++++

== Goals of this lab

In this exercise, we'll focus on the crucial aspect of monitoring AI models once they are deployed in production. Observing and understanding the performance of AI models in real-world scenarios is essential for ensuring their effectiveness and reliability, especially in the context of Parasol's business use cases. This workshop will guide you through confirming that the custom Parasol fine-tuned model is functioning correctly using a simple prompt application. You will then run a load generation workload and use OpenShift AI to observe key performance metrics, including using Prometheus queries. Additionally, you will deploy a smaller, quantized version of the trained model, repeat the workload, and observe performance differences. This hands-on exercise will highlight the importance of continuous monitoring and performance evaluation to maintain high standards of AI model deployment at Parasol Insurance.


== 1. Run podman desktop

Introduction to gen AI + discover and experiment with gen AI models and AI applications on the local desktop, in an inner loop

image::observe/redhat-openshift-ai.png[]


== 1.1. TBD

=== 1.1.1. TBD

== 2. Start a playground, chat with it

== 3. Kill playground, try text summarization recipe, upload claim PDF, view summarization

== 4. Open summarization app (python) in vscode, inspect code (briefly)

== 5. Change the prompt, restart, and observe changes.

== Conclusion

We hope you have enjoyed this module!

Here is a quick summary of what we have learned:

- TBD
- TBD
- TBD